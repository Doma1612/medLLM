This work presented a two-stage approach (retrieval and summarization) for clinical decision support by integrating symptom extraction, document retrieval, and text summarization. Our results indicate that while the model demonstrates a reasonable ability to capture the overall semantic content—as reflected by a comparatively higher BERTScore—the lexical and structural accuracy, as measured by BLEU and ROUGE metrics, leaves considerable room for improvement.\\
One of the key observations is the discrepancy between semantic similarity and exact textual matching. Although the model appears to understand and retain the underlying medical context, it struggles to replicate the precise wording and structure of the reference texts. This divergence is critical in clinical settings, where even subtle changes in phrasing can lead to misinterpretation of important information. Future work should explore strategies to better balance semantic fidelity with lexical accuracy, perhaps by refining the loss function or employing more sophisticated fine-tuning techniques.\\
In our current approach, we used a RoBERTa-based classification model for retrieving relevant papers based on patient symptoms. However, an alternative strategy worth considering is the Retrieval-Augmented Generation (RAG) framework. RAG integrates retrieval directly into the generation process, potentially allowing for a more seamless integration between relevant document context and the generated summaries. This approach could mitigate some of the limitations observed with the RoBERTa classifier, such as overfitting due to the high number of paper labels relative to patient cases. Experimenting with RAG may improve both the accuracy of the retrieval step and the coherence of the final summaries.\\
The overfitting observed during training, likely caused by the high number of paper labels relative to patient cases, further underscores the challenges inherent in this domain. Limited hardware resources imposed additional constraints on batch sizes and training durations, potentially hindering the model’s ability to generalize to unseen data. Scaling up the training process with more robust computational resources or implementing more effective regularization techniques could help mitigate these issues. Additionally, employing a different dataset—ideally one with a more balanced label distribution or higher quality annotations—could further improve training outcomes and enhance the model’s generalizability.\\
The symptom extraction module, which relies on SciSpaCy, performed adequately in capturing medically relevant terms but also produced false positives—misidentifying terms that are contextually ambiguous. While the use of domain-specific pre-trained models like SciSpaCy is a step in the right direction, incorporating additional filtering mechanisms or ensemble approaches may be necessary to enhance accuracy and reduce noise in the extracted data.\\
Despite these challenges, the project's work shows promise. The model’s ability to generate semantically coherent summaries suggests that, with further refinements and possibly the adoption of a RAG-based retrieval strategy, it could become a valuable tool for clinicians overwhelmed by the growing volume of medical literature. Future iterations might benefit from experimenting with more advanced LLMs and larger, more diverse datasets to improve both retrieval and summarization performance.\\
Finally, while standard metrics like BLEU, ROUGE, and METEOR provide useful quantitative insights, they may not fully capture the clinical relevance of the summaries. Incorporating qualitative evaluations from medical professionals and conducting user studies will be essential to validate the system’s practical utility and ensure that the generated summaries meet the high standards required in clinical decision-making.\\
In summary, our work lays the groundwork for leveraging fine-tuned language models in clinical decision support but also highlights several critical areas—such as overfitting, hardware limitations, and symptom extraction accuracy—that need to be addressed. The insights gained from this work will guide future project toward developing more robust and clinically applicable AI tools.
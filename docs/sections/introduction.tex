In the rapidly evolving field of medicine, doctors are required to process vast amounts of information to make well-informed decisions. However, accessing relevant medical literature can be time-consuming and challenging. This project aims to address this issue by developing a large language model designed to assist doctors in gathering crucial patient-related information efficiently.
The proposed model allows doctors to input a patient description as a prompt, after which it retrieves and summarizes relevant medical research papers. This process provides physicians with critical insights derived from medical literature. This approach enhances work efficiency, reduces cognitive load, and ensures that important information is readily accessible when needed.
To achieve this, our system employs a two-stage approach: retrieval and summarization. For the retrieval phase, we utilize \textbf{RoBERTa}~\cite{liu2019robertarobustlyoptimizedbert}, a robust Transformer-based model, to process the input prompt and identify the most relevant medical research articles from the \textbf{PMC-Patients}~\cite{Zhao_2023} database. 
Once the relevant articles are retrieved, the summarization phase is handled using either \textbf{Llama 3.2 1b instruct}\footnote{\url{https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct}} or \textbf{Llama 3.3 70b instruct}\footnote{\url{https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct}}. We experimented with both models separately to determine which one provided the most accurate and useful summaries for clinical decision-making. Through comparative analysis, we assessed which model performed better in summarizing medical literature effectively for our use case.
This report outlines the development, training, and evaluation of the model, detailing its capabilities and limitations. Specifically, we discuss the dataset utilized for training, the preprocessing techniques employed, the symptom extraction process, the machine learning architecture, and the evaluation methods used to assess summarization quality. Furthermore, we examine the modelâ€™s performance and highlight areas for improvement.